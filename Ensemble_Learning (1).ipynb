{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning â€“ Theory Answers\n",
        "\n",
        "## Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "**Answer:**\n",
        "Ensemble Learning is a machine learning approach in which multiple individual models, known as\n",
        "base learners or weak learners, are trained to solve the same problem and their predictions are\n",
        "combined to produce a final output. The key idea behind ensemble learning is that a group of\n",
        "diverse models can collectively perform better than any single model.\n",
        "\n",
        "By aggregating predictions, ensemble methods help reduce errors caused by bias, variance,\n",
        "or noise in the data. This results in improved accuracy, robustness, and generalization\n",
        "performance, especially on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "**Answer:**\n",
        "Bagging (Bootstrap Aggregating) is an ensemble technique where multiple models are trained\n",
        "independently on different bootstrap samples drawn randomly with replacement from the\n",
        "original dataset. The final prediction is obtained by averaging or voting. Bagging primarily\n",
        "focuses on reducing variance and is effective for high-variance models such as decision trees.\n",
        "\n",
        "Boosting, on the other hand, trains models sequentially. Each new model focuses on correcting\n",
        "the mistakes made by the previous models by assigning higher importance to misclassified data\n",
        "points. Boosting mainly reduces bias and can also reduce variance, but it is more sensitive to\n",
        "noise and outliers.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "**Answer:**\n",
        "Bootstrap sampling is a resampling technique where multiple datasets are created by randomly\n",
        "selecting data points from the original dataset with replacement. As a result, some data points\n",
        "may appear multiple times in a sample, while others may not appear at all.\n",
        "\n",
        "In Bagging methods such as Random Forest, bootstrap sampling ensures diversity among the\n",
        "individual decision trees. This diversity reduces correlation among trees, improves prediction\n",
        "stability, and helps prevent overfitting, leading to better overall model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "**Answer:**\n",
        "Out-of-Bag (OOB) samples are the data points that are not selected during the bootstrap sampling\n",
        "process for training a particular model in a Bagging ensemble. On average, about one-third of\n",
        "the data remains unused for each base model.\n",
        "\n",
        "These OOB samples act as a built-in validation set and are used to evaluate the performance of\n",
        "the ensemble model without requiring a separate test dataset. The OOB score provides an\n",
        "unbiased and efficient estimate of model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "**Answer:**\n",
        "In a single Decision Tree, feature importance is calculated based on the reduction in impurity\n",
        "(e.g., Gini index or entropy) at each split. However, these importance values can be unstable\n",
        "because a single tree is highly sensitive to small changes in the training data.\n",
        "\n",
        "In contrast, a Random Forest computes feature importance by averaging importance values across\n",
        "many decision trees. This aggregation results in more stable, reliable, and generalizable feature\n",
        "importance scores, making Random Forests better suited for feature selection and interpretation.\n"
      ],
      "metadata": {
        "id": "ZO-Tt_stc2Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Random Forest on Breast Cancer Dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "top_5_features = feature_importance.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxv0R5dvdArj",
        "outputId": "8801c206-6d68-45bc-ae3e-dd2184656a6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Bagging Classifier vs Single Decision Tree (Iris Dataset)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
        "                        n_estimators=100,\n",
        "                        random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, bag_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEZ6dV66dC4K",
        "outputId": "a337e6cd-063a-4943-a406-d1f9ddff489c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Hyperparameter Tuning using GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UrX00uZdGYI",
        "outputId": "fb4e3d48-843e-4d4a-f9a9-6d6ed109c5f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 100}\n",
            "Best Accuracy: 0.9596180717279925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Bagging Regressor vs Random Forest Regressor\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
        "                           n_estimators=100,\n",
        "                           random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, bag_pred))\n",
        "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFZUadPdQC8",
        "outputId": "024c5a17-b5ae-4ee0-aae0-af4fbfa7c69d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2568358813508342\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Ensemble Learning for Loan Default Prediction\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "As a data scientist predicting loan default, I would follow these steps:\n",
        "\n",
        "1. **Choosing between Bagging or Boosting:**\n",
        "   I would start with Bagging methods like Random Forest if the model suffers from high variance.\n",
        "   If the model shows high bias, I would prefer Boosting methods such as Gradient Boosting or XGBoost.\n",
        "\n",
        "2. **Handling Overfitting:**\n",
        "   Overfitting can be handled using ensemble averaging, limiting tree depth, using cross-validation,\n",
        "   and applying regularization techniques.\n",
        "\n",
        "3. **Selecting Base Models:**\n",
        "   Decision Trees are preferred as base models because they capture non-linear relationships and\n",
        "   work well with ensemble techniques.\n",
        "\n",
        "4. **Evaluating Performance using Cross-Validation:**\n",
        "   I would use k-fold cross-validation to ensure stable performance and avoid data leakage.\n",
        "\n",
        "5. **Why Ensemble Learning Improves Decision-Making:**\n",
        "   Ensemble models combine multiple perspectives, reduce prediction errors, and provide more\n",
        "   robust and reliable predictions, which is critical for financial risk assessment.\n",
        "\n",
        "Overall, ensemble learning improves accuracy, stability, and trust in predictions, leading to\n",
        "better business decisions.\n"
      ],
      "metadata": {
        "id": "X4XRxKv2ZeXl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_N_MsoV6Ze-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}